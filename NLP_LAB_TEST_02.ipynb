{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXxaQrSKj8krpEl4RVsik6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51529/NLP-LAB/blob/main/NLP_LAB_TEST_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFJbjSkziL7G",
        "outputId": "dd7e8c12-bacd-4a8d-99a7-3c01282b1880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Robust Baseline (Target: ~1 min) ---\n",
            "Loading dataset...\n",
            "Starting Grid Search (Cross-Validation)...\n",
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "Best Parameters found: {'multinomialnb__alpha': 0.1, 'tfidfvectorizer__max_features': 10000}\n",
            "Evaluating on Test Set...\n",
            "\n",
            "Accuracy: 0.7122\n",
            "Total Execution Time: 145.99 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def run_robust_baseline():\n",
        "    start_time = time.time()\n",
        "    print(\"--- Starting Robust Baseline (Target: ~1 min) ---\")\n",
        "\n",
        "    # 1. Load Data (Full Dataset)\n",
        "    # removing metadata (headers/footers) makes the task harder and more realistic\n",
        "    print(\"Loading dataset...\")\n",
        "    data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "    # 2. Split Data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 3. Create a Pipeline\n",
        "    # TfidfVectorizer: Weighs words by importance (less frequent = more important)\n",
        "    # ngram_range=(1, 2): Looks at single words AND pairs of words (bi-grams)\n",
        "    pipeline = make_pipeline(\n",
        "        TfidfVectorizer(stop_words='english', ngram_range=(1, 2)),\n",
        "        MultinomialNB()\n",
        "    )\n",
        "\n",
        "    # 4. Grid Search for Hyperparameters (This consumes the most time)\n",
        "    # We test different 'alpha' values to smooth the model\n",
        "    parameters = {\n",
        "        'multinomialnb__alpha': [0.01, 0.1, 1.0],\n",
        "        'tfidfvectorizer__max_features': [5000, 10000]\n",
        "    }\n",
        "\n",
        "    print(\"Starting Grid Search (Cross-Validation)...\")\n",
        "    # cv=3 means it trains the model 3 times for EVERY parameter combination\n",
        "    grid_search = GridSearchCV(pipeline, parameters, cv=3, n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best Parameters found: {grid_search.best_params_}\")\n",
        "\n",
        "    # 5. Evaluate\n",
        "    print(\"Evaluating on Test Set...\")\n",
        "    predicted = grid_search.predict(X_test)\n",
        "    accuracy = grid_search.score(X_test, y_test)\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "    print(f\"Total Execution Time: {duration:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_robust_baseline()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "def run_fast_baseline():\n",
        "    start_time = time.time()\n",
        "    print(\"--- Starting Fast Baseline (Target: < 30 sec) ---\")\n",
        "\n",
        "    # 1. Load Data\n",
        "    # We keep headers/footers here as they often contain \"giveaway\" keywords that make classification easier/faster\n",
        "    print(\"Loading dataset...\")\n",
        "    data = fetch_20newsgroups(subset='all')\n",
        "\n",
        "    # 2. Split Data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 3. Create Simple Pipeline\n",
        "    # CountVectorizer: Just counts word frequency (faster than TF-IDF)\n",
        "    model = make_pipeline(\n",
        "        CountVectorizer(stop_words='english'),\n",
        "        MultinomialNB(alpha=0.1) # Using a static alpha usually works well enough\n",
        "    )\n",
        "\n",
        "    # 4. Fit Model (Single pass, very fast)\n",
        "    print(\"Training Model...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 5. Evaluate\n",
        "    print(\"Evaluating...\")\n",
        "    predicted = model.predict(X_test)\n",
        "    accuracy = model.score(X_test, y_test)\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "    print(f\"Total Execution Time: {duration:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_fast_baseline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLekdsrgkQ3E",
        "outputId": "79809fe6-e39d-43f4-9a79-b48a1aae9c36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Fast Baseline (Target: < 30 sec) ---\n",
            "Loading dataset...\n",
            "Training Model...\n",
            "Evaluating...\n",
            "\n",
            "Accuracy: 0.8918\n",
            "Total Execution Time: 7.36 seconds\n"
          ]
        }
      ]
    }
  ]
}